---
title: "My Experience on Workflows"
institue: "Mersin University, Department of Biotechnology"
subtitle: "aMeta Workhsop"
author: "Emrah Kırdök, Ph.D."
date: "19-04-2023"
format:
    revealjs:
        slide-number: true
        footer: "aMeta Workshop"
        embed-resources: true
---

## Who am I?

+ Emrah Kırdök, Ph.D.
+ Trained as a biologist
+ Working on ancient metagenomics
+ Giving lectures on bioinformatics and data analysis

::: {.notes}

Hi I am Emrah Kırdök. I think most of you already know me. I am working at Mersin University, deparment of biotechnology. Currently, I am working on ancient metagenomics and bioinformatics. Also I am teaching bioinformatics and data analysis at graduate and undergraduate level.

Nora and Zoe asked me to present some of my experiences and struggles in ancient metagenomics studies and how aMeta helped me to deal these. 

After some thought, I've come up with this presentation.

:::

## Outline

+ Ancient metagenomics workflows can be complex
+ Workflowing tools is essential
+ Database problem in ancient metagenomics
+ Benefits of using workflow managers

::: {.notes}

Trained as a biologist, I moved to bioinformatics and ancient metagenomics field after my Ph.D. studies. However, this change was not really easy. 

Bioinformatics is mostly done by trial and error. So, you run something, you observe, you fix and run again. And even ancient metagenomics is particularly hard, since it is still a young filed. There are just a handful of methods out there, and in most occasions you'll have to get your hands dirty.

But there are some tricks and tips that make your life easier in the long run. In this presentation I will explain you my experience on ancient metagenomics from the start and show you some tricks.

:::

## Who will benefit?

+ Early career researchers (MSc, PhD)
+ If you are new to bioinformatics

::: {.notes}

The things that I am going to talk might be a little basic for most of the audience. So, I think early career researchers mostly benefit from this talk. 

If I start again, which way I will work.

:::

## Bioinformatics methdodology

```{dot}
// fig-cap: "A simple representation"

digraph g { 
    rankdir = LR;
    input [label = "Input"];

    workflow [label = "Bioinformatics\nworkflow", shape = "box"];
    output [label = "Output"];
    
    input -> workflow-> output
}
```

::: {.notes}


:::

## Bioinformatics methdodology

Actually it is much more complicated...

```{dot}
// fig-cap: "A simple representation of input and outputs"

digraph g { 
    rankdir = LR;
    input1 [label = "Input1"];
    input2 [label = "Input2"];
    parameters [labels = "Parameters", shape = "parallelogram"];
    tool1 [label = "Tool1", shape = "box"];
    output1 [label = "Output1"];
    output2 [label = "Output2"];

    tool2 [label = "Tool2", shape = "box"];
    output3 [label = "Output3"];
    moreparameters [labels = "More\nParameters", shape = "parallelogram"];


    {input1; input2} -> tool1;
    tool1 -> {output1; output2};
    parameters -> tool1;

    output2 -> tool2 -> output3;
    moreparameters -> tool2;

    {rank = same; parameters; tool1};
    {rank = same; moreparameters; tool2};
}
```

::: {.notes}

The core idea of bioinformatics methodology is to combine several tools into one neat workflow.

But it is much more complicated then that. To do a bionformatics research, you'll need to use a lot of different tools, a lot of files, different paramters.

Often you'll need to optimize parameters to get the correct result.

This is the general idea of bioinformatics work style. 

:::

## Ancient metagenomics methodology

And in ancient metagenomics, it is much more complicated...

```{dot}
// fig-cap: "A simple representation of input and outputs"

digraph g { 
    rankdir = LR;
    fastq [label = "Fastq\nfile"];
    output [label = "Classification\nOutput"]
    classification [label = "Classification", shape = "box"]
    authentication [label = "Extract aDNA reads\nand\nauthenticate", shape = "box"]

    database [labels = "Database", shape = "parallelogram"];

    fastq -> classification -> output;
    database -> classification;
    {rank = same; database; classification};
    output -> authentication;
    database -> authentication;
    authentication -> {authentication_1, authentication_2, authentication_3, authentication_4, authentication_N};
    {authentication_1, authentication_2, authentication_3, authentication_4, authentication_N} -> "Final Report"
} 
```

::: {.notes}

And in ancient metagenomics, this is much more harder. Generally, we classify DNA reads in fastq files using a reference sequence collection. THis collection contains DNA sequences that have known taxonomical ranks. By comparing each sequence to this collection, we try to identify the taxonomic origin of each DNA read.

However, in ancient metaggneomics, we also need to extract DNA sequences and authenticate the ancient status. So, at the end we do not know how many bacteria we have.

:::

## Authentication part of the pipeline

![It would be very hard to authenticate one by one](images/authentication.png)

::: {.notes}

Generally, you will end up with thousands of bacterial species to authenticate. It could be really hard to control and authenticate one by one.
:::

## Ancient metagenomics methodology

It is even more complicated in a real situation:

![The aMeta Workflow](https://kirdoklab.github.io/aMeta-workshop/images/rulegrap_January_2023.png)

## Workflows

+ Workflow, is the materials and methods
+ You need to know all inputs and outputs
+ Bash scripts are quite easy to write
+ But, every time you run it starts from beginning
+ Job dependency?

## My first workflow

![](images/wrapper.png)

::: {.notes}
My first workflow. 

  - very fast
  - completely on bash
  - can preserve job dependencies
  - but it runs from the beginning
:::

## aMeta workflow

+ aMeta allowed me to use `snakemake` in my research
+ A fully robust system that can automatically send slurm jobs
+ Beginning is hard, but you will benefit later
+ But, sometimes can be cryptic

::: {.notes}

You'll have to learn too much things.

:::

## aMeta

![Krakenuniq classification rule as an example](images/snake.png)

::: {.notes}

For example this is the krakenuniq rule in snakemake. You will see the inputs, outputs, and databases clearly.

:::

## Databases

+ Metagenomic database should be big
+ Alignment for aDNA authentication
+ Alignment based databases are quite big!
+ The two step proccess in aMeta
    + memory usage optimization

## Good practices

+ Using snakemake, forces you to follow good practices
+ You will start doing reproducible bioinformatics

## Good practices
```
Project name
├── LICENSE
├── README.md          <- The top-level README
├── data
│   ├── external       <- Data from third party sources.
│   ├── interim        <- Intermediate data that has been transformed.
│   ├── processed      <- The final, canonical data sets for modeling.
│   └── raw            <- The original read only
│
├── docs               <- All the document information should go here
│   ├── reports
│   └── presentations
│
├── workflow           <- Source code and snakemake rules 
│   ├── Snakefile      <- A snakefile, should include all sub rules
│   │
│   ├── rules          <- Seperate snakefile rules
│   │
│   ├── environments   <- Conda environments
│   │
│   └── singularity    <- Singularity containers
│ 
└── results

```

::: {.notes}

A tidy folder keep your research trackable and reproducible.

:::

## Good practices

+ Version controlling
    + git + github
    + also a backup
+ Conda environments
    + Automatically install dependencies

## Using snakemake

+ Forces you to document every step
    + manual modifications
