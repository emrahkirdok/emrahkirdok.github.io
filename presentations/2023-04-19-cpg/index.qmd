---
title: "My Experience on Metagenomics and Workflows"
institue: "Mersin University, Department of Biotechnology"
subtitle: "aMeta Workhsop"
author: "Emrah Kırdök, Ph.D."
date: "19-04-2023"
format:
    revealjs:
        slide-number: true
        footer: "aMeta Workshop"
        embed-resources: true
---

## Who am I?

+ Emrah Kırdök, Ph.D.
+ Trained as a biologist
+ Working on ancient metagenomics
+ Giving lectures on bioinformatics and data analysis

::: {.notes}

Hi I am Emrah Kırdök. I think most of you already know me. I am working at Mersin University, deparment of biotechnology. Currently, I am working on ancient metagenomics and bioinformatics. Also I am teaching bioinformatics and data analysis at graduate and undergraduate level.

Nora and Zoe asked me to present some of my experiences and struggles in ancient metagenomics studies and how aMeta helped me to deal these. 

After some thought, I've come up with this presentation.

:::

## Outline

+ Ancient metagenomics is still a new discipline
+ We do not have very well defined methods
+ Some struggle is inevitable
+ Lessons learnt, some good practice and tricks
+ How aMeta helped me to understand `snakemake`

::: {.notes}

Trained as a biologist, I moved to bioinformatics and ancient metagenomics field after my Ph.D. studies. However, this change was not really easy. 

Bioinformatics is mostly done by trial and error. So, you run something, you observe, you fix and run again. And even ancient metagenomics is particularly hard, since it is still a young filed. There are just a handful of methods out there, and in most occasions you'll have to get your hands dirty.

But there are some tricks and tips that make your life easier in the long run. In this presentation I will explain you my experience on ancient metagenomics from the start and show you some tricks.

:::

# Struggles

## Bioinformatics methdodology

A simple methodology

```{dot}
// fig-cap: "A simple representation of input and outputs"

digraph g { 
    rankdir = LR;
    input1 [label = "Input1"];
    input2 [label = "Input2"];
    parameters [labels = "Parameters", shape = "parallelogram"];
    tool [label = "Tool", shape = "box"];
    output1 [label = "Output1"];
    output2 [label = "Output2"];
    {input1; input2} -> tool;
    tool -> {output1; output2};
    parameters -> tool;
    {rank = same; parameters; tool};
}
```

::: {.notes}

However, generally it is more complicated. You will have more than one input and more then one outputs. Often, you will have some parameters to fine tune.

:::

## Bioinformatics methdodology

Actually it is much more complicated...

```{dot}
// fig-cap: "A simple representation of input and outputs"

digraph g { 
    rankdir = LR;
    input1 [label = "Input1"];
    input2 [label = "Input2"];
    parameters [labels = "Parameters", shape = "parallelogram"];
    tool1 [label = "Tool1", shape = "box"];
    output1 [label = "Output1"];
    output2 [label = "Output2"];

    tool2 [label = "Tool2", shape = "box"];
    output3 [label = "Output3"];
    moreparameters [labels = "More\nParameters", shape = "parallelogram"];


    {input1; input2} -> tool1;
    tool1 -> {output1; output2};
    parameters -> tool1;

    output2 -> tool2 -> output3;
    moreparameters -> tool2;

    {rank = same; parameters; tool1};
    {rank = same; moreparameters; tool2};
}
```

::: {.notes}

The core idea of bioinformatics methodology is to combine several tools into one neat workflow.

But it is much more complicated then that. To do a bionformatics research, you'll need to use a lot of different tools, a lot of files, different paramters.

Often you'll need to optimize parameters to get the correct result.

This is the general idea of bioinformatics work style. 

:::

## Ancient metagenomics methodology

And in ancient metagenomics, it is much more complicated...

```{dot}
// fig-cap: "A simple representation of input and outputs"

digraph g { 
    rankdir = LR;
    fastq [label = "Fastq\nfile"];
    output [label = "Classification\nOutput"]
    classification [label = "Classification", shape = "box"]
    authentication [label = "Extract aDNA reads\nand\nauthenticate", shape = "box"]

    database [labels = "Database", shape = "parallelogram"];

    fastq -> classification -> output;
    database -> classification;
    {rank = same; database; classification};
    output -> authentication;
    database -> authentication;
    authentication -> {authentication_1, authentication_2, authentication_3, authentication_4, authentication_N};
    {authentication_1, authentication_2, authentication_3, authentication_4, authentication_N} -> "Final Report"
} 
```

::: {.notes}

And in ancient metagenomics, this is much more harder. Generally, we classify DNA reads in fastq files using a reference sequence collection. THis collection contains DNA sequences that have known taxonomical ranks. By comparing each sequence to this collection, we try to identify the taxonomic origin of each DNA read.

However, in ancient metaggneomics, we also need to extract DNA sequences and authenticate the ancient status. So, at the end we do not know how many bacteria we have.

:::

## Ancient metagenomics methodology

It is even more complicated in a real situation:

![The aMeta Workflow](https://kirdoklab.github.io/aMeta-workshop/images/rulegrap_January_2023.png)

## Authentication part of the pipeline

![It would be very hard to authenticate one by one](images/authentication.png)

# Workflows

## Workflows

+ Bash scripts are quite easy to write
+ But, every time you run it starts from beginning
+ Every bash script could be specific to one job
+ Job dependency (with slurm?)

## My first workflow

![](images/wrapper.png)

::: {.notes}
My first workflow. 

  - very fast
  - completely on bash
  - can preserve job dependencies
  - but it runs from the beginning
:::

## Snakemake

+ aMeta allowed me to learn `snakemake`
+ Snakemake changed my view
+ A fully robust system that can automatically send slurm jobs

## Snakemake

![](images/snake.png)


# Extras

## Helps you to keep a tidy folder

```
Project name
├── LICENSE
├── README.md          <- The top-level README for developers using this project.
├── data
│   ├── external       <- Data from third party sources.
│   ├── interim        <- Intermediate data that has been transformed.
│   ├── processed      <- The final, canonical data sets for modeling.
│   └── raw            <- The original read only
│
├── docs               <- All the document information should go here
│   ├── reports
│   └── presentations
│
├── workflow           <- Source code and snakemake rules 
│   ├── Snakefile      <- A snakefile, should include all sub rules
│   │
│   ├── rules          <- Seperate snakefile rules
│   │
│   ├── environments   <- Conda environments
│   │
│   └── singularity    <- Singularity containers
│ 
└── results

```

::: {.notes}

A tidy folder keep your research trackable and reproducible.

:::

## Available on Github

+ Version controlling 
+ Share an collaborate
+ Also help you to backup

